# 大模型与机器人操作

## 机械臂操作

- [RDT-1B](https://rdt-robotics.github.io/rdt-robotics/) - 双手操作的扩散基础模型
- [ReflectVLM](https://github.com/yunhaif/reflect-vlm) - 多阶段长视界机器人操作的视觉语言模型
- [AnyPlace](https://any-place.github.io/) - 学习机器人操作的广义物体放置
- [RoboPoint](https://robo-point.github.io/) - 用于机器人空间可供性预测的视觉语言模型
- [ROSA](https://github.com/nasa-jpl/rosa) - ROS1 和 ROS2 系统的 AI 助手
- [RAI](https://github.com/RobotecAI/rai) - 一个用于机器人的多供应商代理框架
- [VLABench](https://github.com/OpenMOSS/VLABench) - 具有长期推理任务的语言条件机器人操作的大规模基准
- [RoCo](https://github.com/MandiZhao/robot-collab.git) - 具有大型语言模型的辩证多机器人协作
- [RoboFactory](https://iranqin.github.io/robofactory/) - 探索具有组合约束的具身代理协作

## 人机交互与操作

- [PhysHOI](https://github.com/wyhuai/PhysHOI) - 基于物理的动态人机交互模拟
- [CooHOI](https://github.com/Winston-Gu/CooHOI) - 通过操纵物体动力学学习人与物体的协同交互
- [HOIFHLI](https://hoifhli.github.io/) - 通过人类层面的指令实现人与物体的交互
- [InterMimic](https://sirui-xu.github.io/InterMimic/) - 面向基于物理的人与物体交互的通用全身控制
- [BiGym](https://github.com/chernyadev/bigym) - 移动双手演示驱动机器人操作的新基准和学习环境
  - [Aloha Bigym](https://github.com/AlmondGod/aloha-bigym.git) - ALOHA 演示驱动的移动双手操作基准测试
- [Open-TeleVision](https://github.com/OpenTeleVision/TeleVision) - 具有沉浸式主动视觉反馈的远程操作

## 生成式机器人学习

- [RoboGen](https://github.com/Genesis-Embodied-AI/RoboGen) - 通过生成模拟释放无限数据，实现机器人自动学习
- [Genesis](https://genesis-embodied-ai.github.io/) - 用于机器人及其他领域的生成式通用物理引擎
- [RL Zero](https://hari-sikchi.github.io/rlzero/) - 无需任何监督的零样本语言到行为
- [GVHMR](https://github.com/zju3dv/GVHMR) - 从普通视频中还原出人物真实3D动作
- [Body Transformer](https://github.com/carlosferrazza/BodyTransformer) - 利用机器人化身进行策略学习
- [CLoSD](https://github.com/GuyTevet/CLoSD) - 通过结合文本驱动的扩散生成和强化学习实现人物动作的生成和执行
- [Unified World Models](https://weirdlabuw.github.io/uwm/) - 结合视频和动作扩散在大型机器人数据集上的预训练
