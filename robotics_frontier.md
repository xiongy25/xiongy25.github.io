# 机器人前沿技术

## 机器人框架与工具

- [Rofunc](https://github.com/Skylark0924/Rofunc) - 机器人演示和操作的全流程 Python 包
- [skrl](https://github.com/Toni-SM/skrl) - 模块化强化学习库
- [DIAL-MPC](https://lecar-lab.github.io/dial-mpc/) - 基于全阶采样的MPC通过扩散式退火实现扭矩级运动控制
- [One Policy To Run Them All](https://github.com/nico-bohlinger/one_policy_to_run_them_all.git) - 一个策略解决所有问题：多实体运动的端到端学习方法
- [PARTNR](https://github.com/facebookresearch/partnr-planner) - 具身多智能体任务中的规划和推理基准
- [RAI](https://github.com/RobotecAI/rai?tab=readme-ov-file#simulation-demos) - 一个用于机器人的多供应商代理框架
- [ROSA](https://github.com/nasa-jpl/rosa) - ROS1 和 ROS2 系统的 AI 助手
- [robosuite](https://github.com/ARISE-Initiative/robosuite) - 机器人学习的模块化模拟框架和基准
- [DEMO](https://adrialopezescoriza.github.io/demo3/) - 具有演示增强奖励、策略和世界模型学习的多阶段操控

## 动作生成与控制

- [MaskedMimic](https://research.nvidia.com/labs/par/maskedmimic/) - 基于物理的统一角色控制方法
- [ProtoMotions](https://github.com/NVlabs/ProtoMotions/tree/main) - 基于物理的角色动画
- [PDP](https://stanford-tml.github.io/PDP.github.io/) - 基于物理的扩散策略角色动画
- [GVHMR](https://github.com/zju3dv/GVHMR) - 从普通视频中还原出人物真实3D动作
- [CLoSD](https://github.com/GuyTevet/CLoSD?tab=readme-ov-file) - 通过结合文本驱动的扩散生成和强化学习（RL）来实现人物动作的生成和执行
- [UH-1](https://usc-gvl.github.io/UH-1/) - 通过大量人体视频进行学习，实现通用人形姿势控制
- [HOVER](https://hover-versatile-humanoid.github.io/) - 人形机器人的多功能神经全身控制器
- [BeamDojo](https://why618188.github.io/beamdojo/) - 学习在稀疏立足点上进行敏捷的人形运动
- [Embrace Collisions](https://project-instinct.github.io/) - 可部署的接触无关运动的人形阴影技术
- [OmniH2O](https://omni.human2humanoid.com/?login=from_csdn) - 通用灵巧的人机全身远程操作和学习
- [InterMimic](https://sirui-xu.github.io/InterMimic/) - 面向基于物理的人与物体交互的通用全身控制
- [Exbody2](https://exbody2.github.io/) - 高级富有表现力的人形全身控制
- [recipe](https://toruowo.github.io/recipe/) - 基于视觉的人形机器人灵巧操作的 模拟到现实强化学习
- [HumanUP](https://humanoid-getup.github.io/) - 学习现实世界人形机器人的起身策略
- [GRUtopia](https://github.com/OpenRobotLab/GRUtopia.git) - 梦想中的城市通用机器人
- [Humanoid Policy](https://github.com/RogerQi/human-policy.git) - 使用头戴摄像头捕捉数据然后用于训练机器人
- [Puffer PHC](https://github.com/kywch/puffer-phc.git) - 简化版 Pufferlib、CARBS 的永久性人形控制
- [FLAM](https://xianqi-zhang.github.io/FLAM/) - 基于基础模型的人形机器人步态和操作稳定化

## 多智能体系统

- [CooHOI](https://github.com/Winston-Gu/CooHOI) - 通过操纵物体动力学学习人与物体的协同交互
- [PARTNR](https://github.com/facebookresearch/partnr-planner) - 具身多智能体任务中的规划和推理基准

## 资源与教程

- [Simulately](https://github.com/geng-haoran/Simulately) - 一个收集物理模拟器有用信息的网站
- [VLABench](https://github.com/OpenMOSS/VLABench) - 具有长期推理任务的语言条件机器人操作的大规模基准
- [MuJoCo Playground](https://playground.mujoco.org/) - 用于 GPU 加速机器人学习和模拟到现实转换的开源库
- [Cosmos](https://github.com/NVIDIA/Cosmos.git) - 世界模型开发平台
- [Eurekaverse](https://github.com/eureka-research/eurekaverse) - 通过大型语言模型生成环境课程
- [Genesis](https://genesis-embodied-ai.github.io/) - 用于机器人及其他领域的生成式通用物理引擎
- [RoboGen](https://github.com/Genesis-Embodied-AI/RoboGen) - 通过生成模拟释放无限数据，实现机器人自动学习
- [ASAP](https://agile.human2humanoid.com/) - 协调模拟和现实世界的物理学习敏捷的人形全身技能
- [RL Zero](https://hari-sikchi.github.io/rlzero/) - 无需任何监督的零样本语言到行为
- [BiGym](https://github.com/chernyadev/bigym) - 移动双手演示驱动机器人操作的新基准和学习环境
- [Aloha Bigym](https://github.com/AlmondGod/aloha-bigym.git) - ALOHA 演示驱动的移动双手操作基准测试
- [Open-TeleVision](https://github.com/OpenTeleVision/TeleVision) - 具有沉浸式主动视觉反馈的远程操作
- [PIM](https://junfeng-long.github.io/PIM/) - 利用感知内部模型学习人形运动
- [RoboPoint](https://robo-point.github.io/) - 用于机器人空间可供性预测的视觉语言模型
- [PhysHOI](https://github.com/wyhuai/PhysHOI) - 基于物理的动态人机交互模拟
- [PHC](https://github.com/ZhengyiLuo/PHC) - 一种基于物理的人形控制器
- [SplatSim](https://splatsim.github.io/) - 使用高斯分层进行 RGB 操作策略的零样本 Sim2Real 移
- [ManiSkill](https://github.com/haosulab/ManiSkill) - 高性能 GPU 并行化机器人模拟器
- [基于视觉的人形机器人灵巧操作的模拟到现实强化学习](https://toruowo.github.io/recipe/) - 
- [机器人周报精选] (https://github.com/msadowski/awesome-weekly-robotics.git) - 

## 开源数据集

- [Isaac-GR00T](https://github.com/NVIDIA/Isaac-GR00T.git) - NVIDIA Isaac GR00T N1 是世界上第一个用于通用人形机器人推理和技能的开放基础模型
- [unitree开源全身运动数据集](https://huggingface.co/datasets/unitreerobotics/LAFAN1_Retargeting_Dataset) -  Unitree H1、H1-2 和 G1 人形机器人的动作捕捉数据，使用数值优化方法使它们的运动轨迹更加自然